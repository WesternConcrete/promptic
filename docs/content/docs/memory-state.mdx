---
title: "Memory & State"
group: "Advanced Usage"
groupOrder: 0
---

By default, each function call is independent and stateless. Setting `memory=True` enables built-in conversation memory, allowing the LLM to maintain context across multiple interactions.

Here's a practical example using Gradio to create a web-based chatbot interface:

```python
import gradio as gr
from promptic import llm

@llm(memory=True, stream=True)
def assistant(message):
    """{message}"""

def predict(message, history):
    partial_message = ""
    for chunk in assistant(message):
        partial_message += str(chunk)
        yield partial_message

with gr.ChatInterface(title="Promptic Chatbot Demo", fn=predict) as demo:
    # ensure clearing the chat window clears the chat history
    demo.chatbot.clear(assistant.clear)

demo.launch()
```

## Custom Storage Solutions

For custom storage solutions, you can extend the `State` class to implement persistence in any database or storage system:

```python
import json
from promptic import State, llm

class RedisState(State):
    def __init__(self, redis_client):
        super().__init__()
        self.redis = redis_client
        self.key = "chat_history"
    
    def add_message(self, message):
        self.redis.rpush(self.key, json.dumps(message))
    
    def get_messages(self, limit=None):
        messages = self.redis.lrange(self.key, 0, -1)
        return [json.loads(m) for m in messages][-limit:] if limit else messages
    
    def clear(self):
        self.redis.delete(self.key)

@llm(state=RedisState(redis_client))
def persistent_chat(message):
    """Chat: {message}"""
``` 